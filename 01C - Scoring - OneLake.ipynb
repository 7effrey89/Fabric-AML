{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Saving Predictions in Onelake\n",
        "**Note**: Run this notebook in Azure Machine Learning Serverless Spark Compute\n",
        "\n",
        "This sample code needs to be **executed with the serverless spark compute** in Azure Machine Learning as the standard compute instance doesn't have Java installed (required for saving as delta tables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1763476731911
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "session_starting",
              "parent_msg_id": "566391b0-9239-4bb4-b14a-d882cc276988",
              "queued_time": "2025-11-20T08:39:00.1239254Z",
              "session_id": null,
              "session_start_time": "2025-11-20T08:39:00.125127Z",
              "spark_jobs": null,
              "spark_pool": null,
              "state": "session_starting",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Execute through Serveress Spark Compute\n",
        "\n",
        "# Install required packages:\n",
        "# %pip install azure-ai-ml azure-identity pyspark delta-spark\n",
        "import json\n",
        "import pandas as pd\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import ClientSecretCredential\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Service Principal Credentials\n",
        "# ---------------------------\n",
        "tenant_id = \"<tenant-id>\"\n",
        "client_id = \"<client-id>\"\n",
        "client_secret = \"<client-secret>\"\n",
        "\n",
        "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. AML Workspace Details\n",
        "# ---------------------------\n",
        "SUBSCRIPTION = \"<subscription-id>\"\n",
        "RESOURCE_GROUP = \"rg-we-atpws-aml\"\n",
        "WS_NAME = \"aml-ws-atp001\"\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")\n",
        "\n",
        "\n",
        "# Endpoint configuration\n",
        "online_endpoint_name = \"credit-endpoint-78a2c255\" #hardcoded AML online endpoint for scoring/prediction\n",
        "\n",
        "# Step 1: Load your CSV file from datastore\n",
        "input_path = \"abfss://UnifiedData@onelake.dfs.fabric.microsoft.com/maag_bronze.Lakehouse/Files/raw-files/for-batch-scoring/data_with_headers.csv\" #this is training csv file in a lakehouse\n",
        "\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Step 2: Convert DataFrame to the endpoint's expected JSON format\n",
        "request_data = {\n",
        "    \"input_data\": {\n",
        "        \"columns\": list(range(len(df.columns))),\n",
        "        \"index\": list(range(len(df))),\n",
        "        \"data\": df.values.tolist()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 3: Save as temporary JSON file\n",
        "temp_request_file = \"./batch_request.json\" #\n",
        "with open(temp_request_file, \"w\") as f:\n",
        "    json.dump(request_data, f)\n",
        "\n",
        "\n",
        "# Step 4: Invoke endpoint for batch scoring\n",
        "result = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    request_file=temp_request_file,\n",
        "    deployment_name=\"blue\",\n",
        ")\n",
        "\n",
        "# Step 5: Parse predictions\n",
        "try:\n",
        "    predictions = json.loads(result)\n",
        "except json.JSONDecodeError:\n",
        "    predictions = eval(result)\n",
        "\n",
        "# Step 6: Add predictions to original DataFrame\n",
        "df['prediction'] = predictions\n",
        "\n",
        "# ---------------------------\n",
        "# 8. Save as Delta Table in OneLake\n",
        "# ---------------------------\n",
        "\n",
        "# Convert Pandas DataFrame (df) to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "delta_path = \"abfss://UnifiedData@onelake.dfs.fabric.microsoft.com/maag_gold.Lakehouse/Tables/dbo/azureml_card_cardit_scoring\"  #the prediction to be saved as d delta table\n",
        "\n",
        "spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
        "\n",
        "print(f\"âœ… Model card saved as Delta table at: {delta_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "codemirror_mode": "ipython",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython",
      "version": "3.8.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
